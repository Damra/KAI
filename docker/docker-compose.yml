services:
  # ── Ollama — Local LLM with GPU passthrough ──
  ollama:
    image: ollama/ollama:latest
    ports:
      - "11434:11434"
    volumes:
      - ollama_data:/root/.ollama
    environment:
      - OLLAMA_MAX_VRAM=3072            # VRAM limit: ~50% of 6GB
      - NVIDIA_VISIBLE_DEVICES=0
      - OLLAMA_NUM_PARALLEL=1
      - OLLAMA_CONTEXT_LENGTH=2048      # Daha kısa context = daha hızlı yanıt
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: 1
              capabilities: [gpu]
        limits:
          memory: 4G
    healthcheck:
      test: ["CMD", "ollama", "list"]
      interval: 10s
      timeout: 5s
      retries: 10
    restart: unless-stopped

  # ── Ollama Init — Pull models after Ollama is healthy ──
  ollama-init:
    image: curlimages/curl:latest
    depends_on:
      ollama:
        condition: service_healthy
    entrypoint: ["/bin/sh", "-c"]
    command:
      - |
        echo "Pulling qwen2.5-coder:3b..."
        curl -sf http://ollama:11434/api/pull -d '{"name":"qwen2.5-coder:3b"}' || true
        echo "Pulling nomic-embed-text..."
        curl -sf http://ollama:11434/api/pull -d '{"name":"nomic-embed-text"}' || true
        echo "Models ready."
    restart: "no"

  # ── PostgreSQL + pgvector — Episodic memory ──
  postgres:
    image: pgvector/pgvector:pg16
    environment:
      POSTGRES_DB: kai
      POSTGRES_USER: ${POSTGRES_USER:-kai}
      POSTGRES_PASSWORD: ${POSTGRES_PASSWORD:-kai_secret}
    ports:
      - "5432:5432"
    volumes:
      - pgdata:/var/lib/postgresql/data
    healthcheck:
      test: ["CMD-SHELL", "pg_isready -U kai"]
      interval: 5s
      timeout: 5s
      retries: 20
      start_period: 30s
    restart: unless-stopped

  # ── Neo4j — Semantic memory (knowledge graph) ──
  neo4j:
    image: neo4j:5-community
    environment:
      NEO4J_AUTH: ${NEO4J_USER:-neo4j}/${NEO4J_PASSWORD:-kai_secret}
      NEO4J_PLUGINS: '["apoc"]'
    ports:
      - "7474:7474"
      - "7687:7687"
    volumes:
      - neo4jdata:/data
    healthcheck:
      test: ["CMD", "neo4j", "status"]
      interval: 10s
      timeout: 10s
      retries: 20
      start_period: 30s
    restart: unless-stopped

  # ── KAI Backend — Multi-agent code generation ──
  kai-app:
    build:
      context: ..
      dockerfile: docker/Dockerfile
    ports:
      - "8080:8080"
    environment:
      - LLM_PROVIDER=${LLM_PROVIDER:-ollama}
      - OLLAMA_URL=http://ollama:11434
      - OLLAMA_MODEL=${OLLAMA_MODEL:-qwen2.5-coder:3b}
      - ANTHROPIC_API_KEY=${ANTHROPIC_API_KEY:-}
      - VOYAGE_API_KEY=${VOYAGE_API_KEY:-}
      - TAVILY_API_KEY=${TAVILY_API_KEY:-}
      - LLM_MODEL=${LLM_MODEL:-claude-sonnet-4-5-20250929}
      - POSTGRES_URL=jdbc:postgresql://postgres:5432/kai
      - POSTGRES_USER=${POSTGRES_USER:-kai}
      - POSTGRES_PASSWORD=${POSTGRES_PASSWORD:-kai_secret}
      - NEO4J_URL=bolt://neo4j:7687
      - NEO4J_USER=${NEO4J_USER:-neo4j}
      - NEO4J_PASSWORD=${NEO4J_PASSWORD:-kai_secret}
      - KAI_API_KEY=${KAI_API_KEY:-dev-key}
      - SANDBOX_PATH=/tmp/kai-sandbox
    depends_on:
      postgres:
        condition: service_healthy
      neo4j:
        condition: service_healthy
      ollama-init:
        condition: service_completed_successfully
    restart: unless-stopped

  # ── Frontend — React SPA served by nginx ──
  frontend:
    build:
      context: ../frontend
      dockerfile: Dockerfile
    ports:
      - "3000:80"
    depends_on:
      - kai-app
    restart: unless-stopped

volumes:
  ollama_data:
  pgdata:
  neo4jdata:
